{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dual Approach to Sparse Regression\n",
    "## A Comparison with LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "We have seen in class different formulations of robust and sparse regression. In particular, we saw how LASSO regression can be interpreted as a particular instance of robust regression, and we have seen both a primal and a dual approach to sparse regression. In this recitation, we will implement the dual approach, and compare it with LASSO regression.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The Dual Method\n",
    "\n",
    "We showed in the slides that our original problem,\n",
    "\n",
    "\\begin{align*}\n",
    "&\\min_\\boldsymbol\\beta \\frac{1}{2}\\| \\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta \\|_2^2+\\frac{1}{2\\gamma}\\|\\boldsymbol\\beta \\|_2^2.\\\\\n",
    "&\\text{s.t.}\\; \\|\\boldsymbol\\beta\\|_0\\leq k\n",
    "\\end{align*}\n",
    " can be transformed to:\n",
    "\\begin{align*}\n",
    "&\\min_{\\boldsymbol s \\in S_k} \\frac{1}{2}\\mathbf{y}^T\\left(\\mathbf{I}_n+\\gamma\\sum_i s_i\\mathbf{K}_i\\right)^{-1}\\mathbf{y}\n",
    "\\end{align*}\n",
    "where $K_i=X_iX_i^T$, and $X_i$ is the $i$th column of the feature matrix. Now we have a convex binary problem that is convex, as it is a composition of convex functions. Therefore, we can utilize the cutting plane method to solve it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gurobi, StatsBase, DataFrames, JuMP, LinearAlgebra, Distributions,Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cutting Plane\n",
    "\n",
    "To use the cutting plane method to solve this problem, we need to know how to quickly calculate the value and the derivative of the function:\n",
    "\\begin{align*}\n",
    "c(\\mathbf{s})=&\\frac{1}{2}\\mathbf{y}^T\\left(\\mathbf{I}_n+\\gamma\\sum_i s_i\\mathbf{K}_i\\right)^{-1}\\mathbf{y}\n",
    "\\end{align*}\n",
    "With respect to $\\mathbf{s}$ in the minimization problem above. Some clever algebra leads to the solution that:\n",
    "\\begin{align*}\n",
    "&\\nabla c(\\mathbf{s})_j = -\\frac{\\gamma}{2}\\alpha^{*T}X_jX_j^T\\alpha^*\n",
    "\\end{align*}\n",
    "Where $\\alpha^*=\\left(\\mathbf{I}_n-\\mathbf{X}_s\\left(\\frac{\\mathbf{I}_k}{\\gamma}+\\mathbf{X}_s^T\\mathbf{X}_s\\right)^{-1}\\mathbf{X}_s^T\\right)\\mathbf{y}$. The Matrix Inversion Lemma further tells us that:\n",
    "$$\n",
    "\\left(\\mathbf{I}_n+\\gamma\\sum_i s_i\\mathbf{K}_i\\right)^{-1}=\\left(\\mathbf{I}_n-\\mathbf{X}_s\\left(\\frac{\\mathbf{I}_k}{\\gamma}+\\mathbf{X}_s^T\\mathbf{X}_s\\right)^{-1}\\mathbf{X}_s^T\\right)\n",
    "$$\n",
    "So $c(\\mathbf{s})=\\frac{1}{2}y^T\\alpha^*$.\n",
    "\n",
    "Therefore, we have:\n",
    "\\begin{align*}\n",
    "c(\\mathbf{s})&=\\frac{1}{2}y^T\\alpha^*\\\\\n",
    "\\nabla c(\\mathbf{s})_j &= -\\frac{\\gamma}{2}\\alpha^{*T}X_jX_j^T\\alpha^*\n",
    "\\end{align*}\n",
    "We implement this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regobj (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function regobj(X,Y,s,γ)\n",
    "    indices = findall(s .> 0.5)\n",
    "    n = length(Y)\n",
    "    denom = 2n\n",
    "    Xs = X[:, indices]\n",
    "    α = Y - Xs * (inv(I / γ + Xs' * Xs) * (Xs'* Y))\n",
    "    val = dot(Y, α) / denom\n",
    "    tmp = X' * α\n",
    "    ∇s = -γ .* tmp .^ 2 ./ denom\n",
    "  return val, ∇s\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Model\n",
    "\n",
    "With this helper function, we will now start building the dual method for sparse regression. \n",
    "\n",
    "#### Step 1: Define the Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$ t $$"
      ],
      "text/plain": [
       "t"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 100\n",
    "Random.seed!(15095)\n",
    "X = rand(1000,100)\n",
    "Random.seed!(15095)\n",
    "Y = rand(1000)\n",
    "k = 5\n",
    "γ = 100\n",
    "miop = Model(Gurobi.Optimizer)\n",
    "# Optimization variables\n",
    "@variable(miop, s[1:p], Bin)\n",
    "@variable(miop, t >= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Set Up Constraints and Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ t $$"
      ],
      "text/plain": [
       "t"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constraints\n",
    "@constraint(miop, sum(s) <= k)\n",
    "s0 = zeros(p)\n",
    "s0[1:k] .= 1\n",
    "p0, ∇s0 = regobj(X,Y, s0, γ)\n",
    "@constraint(miop, t >= p0 + dot(∇s0, s - s0))\n",
    "# objective\n",
    "\n",
    "@objective(miop, Min, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the Cutting Plane Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outer_approximation (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function outer_approximation(cb_data)\n",
    "    s_val = []\n",
    "    for i = 1:p\n",
    "        s_val = [s_val;callback_value(cb_data, s[i])]\n",
    "    end\n",
    "    #s_val = callback_value.(cb_data, s)\n",
    "    obj, ∇s = regobj(X,Y, s_val, γ)\n",
    "    # add the cut: t >= obj + sum(∇s * (s - s_val))\n",
    "    offset = sum(∇s .* s_val)\n",
    "    con = @build_constraint( t >= obj + sum(∇s[j] * s[j] for j=1:p) - offset)\n",
    "    MOI.submit(miop, MOI.LazyConstraint(cb_data), con)\n",
    "    #@lazyconstraint(cb, t >= obj + sum(∇s[j] * s[j] for j=1:p) - offset)\n",
    "end\n",
    "MOI.set(miop, MOI.LazyConstraintCallback(), outer_approximation)\n",
    "\n",
    "#s_val = JuMP.value.(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Modularize the Code into a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseRegression (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function SparseRegression(X,Y,γ,k; solver_output=0)\n",
    "    p=size(X)[2]\n",
    "    miop = Model(Gurobi.Optimizer)\n",
    "    set_optimizer_attribute(miop, \"OutputFlag\", solver_output) \n",
    "    # Optimization variables\n",
    "    @variable(miop, s[1:p], Bin)\n",
    "    @variable(miop, t >= 0)\n",
    "    # Objective\n",
    "    @objective(miop, Min, t)\n",
    "    # Constraints\n",
    "    @constraint(miop, sum(s) <= k)\n",
    "    s0 = zeros(p)\n",
    "    s0[1:k] .= 1\n",
    "    p0, ∇s0 = regobj(X, Y, s0, γ)\n",
    "    @constraint(miop, t >= p0 + dot(∇s0, s - s0))\n",
    "    function outer_approximation(cb_data)\n",
    "        s_val = []\n",
    "        for i = 1:p\n",
    "            s_val = [s_val;callback_value(cb_data, s[i])]\n",
    "        end\n",
    "        #s_val = callback_value.(cb_data, s)\n",
    "        obj, ∇s = regobj(X,Y, s_val, γ)\n",
    "        # add the cut: t >= obj + sum(∇s * (s - s_val))\n",
    "        offset = sum(∇s .* s_val)\n",
    "        con = @build_constraint( t >= obj + sum(∇s[j] * s[j] for j=1:p) - offset)\n",
    "        MOI.submit(miop, MOI.LazyConstraint(cb_data), con)\n",
    "        #@lazyconstraint(cb, t >= obj + sum(∇s[j] * s[j] for j=1:p) - offset)\n",
    "    end\n",
    "    MOI.set(miop, MOI.LazyConstraintCallback(), outer_approximation)\n",
    "    # solving the actual model\n",
    "    optimize!(miop)\n",
    "    s_opt = JuMP.value.(s)\n",
    "    s_nonzeros = findall(x -> x>0.5, s_opt)\n",
    "    β = zeros(p)\n",
    "    X_s = X[:, s_nonzeros]\n",
    "    # formula for the nonzero coefficients\n",
    "    β[s_nonzeros] = γ * X_s' * (Y - X_s * ((I / γ + X_s' * X_s) \\ (X_s'* Y)))\n",
    "    return s_opt, β,s_nonzeros\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Load Data to Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Column1</th><th>Column2</th><th>Column3</th><th>Column4</th><th>Column5</th><th>Column6</th><th>Column7</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>78 rows × 7 columns</p><tr><th>1</th><td>6.4673</td><td>225.1</td><td>78.2</td><td>55.0</td><td>29.47</td><td>1.26</td><td>47.9</td></tr><tr><th>2</th><td>9.4293</td><td>242.4</td><td>65.5</td><td>66.1</td><td>28.51</td><td>0.76</td><td>23.4</td></tr><tr><th>3</th><td>9.4986</td><td>242.4</td><td>70.7</td><td>67.7</td><td>28.64</td><td>0.7</td><td>48.4</td></tr><tr><th>4</th><td>9.476</td><td>257.1</td><td>67.3</td><td>68.2</td><td>29.08</td><td>0.77</td><td>37.3</td></tr><tr><th>5</th><td>7.6488</td><td>238.1</td><td>71.7</td><td>57.2</td><td>27.58</td><td>0.95</td><td>38.6</td></tr><tr><th>6</th><td>7.3895</td><td>261.1</td><td>60.9</td><td>62.3</td><td>29.34</td><td>1.02</td><td>33.9</td></tr><tr><th>7</th><td>9.1652</td><td>261.1</td><td>68.7</td><td>65.3</td><td>28.14</td><td>1.14</td><td>40.2</td></tr><tr><th>8</th><td>7.4426</td><td>252.0</td><td>70.7</td><td>62.3</td><td>29.67</td><td>1.16</td><td>39.7</td></tr><tr><th>9</th><td>7.3775</td><td>238.2</td><td>65.3</td><td>61.4</td><td>30.04</td><td>1.05</td><td>36.2</td></tr><tr><th>10</th><td>7.8969</td><td>251.0</td><td>67.4</td><td>63.0</td><td>28.83</td><td>0.83</td><td>34.5</td></tr><tr><th>11</th><td>8.755</td><td>242.6</td><td>73.8</td><td>65.8</td><td>28.72</td><td>0.9</td><td>33.3</td></tr><tr><th>12</th><td>7.7309</td><td>248.2</td><td>60.3</td><td>64.2</td><td>30.13</td><td>1.23</td><td>44.6</td></tr><tr><th>13</th><td>9.4388</td><td>256.6</td><td>61.3</td><td>68.1</td><td>28.62</td><td>0.91</td><td>33.9</td></tr><tr><th>14</th><td>9.0225</td><td>265.5</td><td>59.8</td><td>63.8</td><td>28.14</td><td>0.9</td><td>39.1</td></tr><tr><th>15</th><td>7.1945</td><td>235.4</td><td>58.3</td><td>57.6</td><td>29.88</td><td>1.53</td><td>39.3</td></tr><tr><th>16</th><td>7.7147</td><td>240.8</td><td>71.4</td><td>59.7</td><td>28.48</td><td>0.74</td><td>41.7</td></tr><tr><th>17</th><td>8.3082</td><td>248.8</td><td>60.5</td><td>61.8</td><td>28.52</td><td>0.93</td><td>21.2</td></tr><tr><th>18</th><td>10.1185</td><td>251.1</td><td>70.6</td><td>69.1</td><td>28.11</td><td>0.91</td><td>48.4</td></tr><tr><th>19</th><td>7.0209</td><td>240.9</td><td>66.5</td><td>62.7</td><td>29.78</td><td>0.75</td><td>31.6</td></tr><tr><th>20</th><td>8.0647</td><td>253.6</td><td>64.3</td><td>62.4</td><td>28.46</td><td>1.01</td><td>42.6</td></tr><tr><th>21</th><td>7.7155</td><td>249.9</td><td>63.5</td><td>67.9</td><td>30.79</td><td>0.86</td><td>29.2</td></tr><tr><th>22</th><td>7.7808</td><td>249.0</td><td>74.0</td><td>65.0</td><td>30.2</td><td>0.76</td><td>32.4</td></tr><tr><th>23</th><td>7.0081</td><td>232.8</td><td>75.5</td><td>54.7</td><td>28.43</td><td>1.16</td><td>40.0</td></tr><tr><th>24</th><td>8.1518</td><td>242.5</td><td>69.3</td><td>60.9</td><td>27.78</td><td>1.0</td><td>37.0</td></tr><tr><th>25</th><td>7.5486</td><td>241.9</td><td>76.7</td><td>65.2</td><td>30.74</td><td>0.73</td><td>35.6</td></tr><tr><th>26</th><td>6.9462</td><td>228.1</td><td>70.4</td><td>59.0</td><td>30.22</td><td>1.0</td><td>51.4</td></tr><tr><th>27</th><td>9.8701</td><td>253.8</td><td>62.7</td><td>68.2</td><td>29.36</td><td>0.66</td><td>38.8</td></tr><tr><th>28</th><td>9.2063</td><td>246.3</td><td>69.0</td><td>66.4</td><td>28.17</td><td>0.81</td><td>37.3</td></tr><tr><th>29</th><td>10.6124</td><td>269.3</td><td>66.4</td><td>71.6</td><td>28.31</td><td>0.94</td><td>50.0</td></tr><tr><th>30</th><td>7.3346</td><td>238.7</td><td>70.6</td><td>58.3</td><td>29.08</td><td>0.86</td><td>43.9</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& Column1 & Column2 & Column3 & Column4 & Column5 & Column6 & Column7\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 6.4673 & 225.1 & 78.2 & 55.0 & 29.47 & 1.26 & 47.9 \\\\\n",
       "\t2 & 9.4293 & 242.4 & 65.5 & 66.1 & 28.51 & 0.76 & 23.4 \\\\\n",
       "\t3 & 9.4986 & 242.4 & 70.7 & 67.7 & 28.64 & 0.7 & 48.4 \\\\\n",
       "\t4 & 9.476 & 257.1 & 67.3 & 68.2 & 29.08 & 0.77 & 37.3 \\\\\n",
       "\t5 & 7.6488 & 238.1 & 71.7 & 57.2 & 27.58 & 0.95 & 38.6 \\\\\n",
       "\t6 & 7.3895 & 261.1 & 60.9 & 62.3 & 29.34 & 1.02 & 33.9 \\\\\n",
       "\t7 & 9.1652 & 261.1 & 68.7 & 65.3 & 28.14 & 1.14 & 40.2 \\\\\n",
       "\t8 & 7.4426 & 252.0 & 70.7 & 62.3 & 29.67 & 1.16 & 39.7 \\\\\n",
       "\t9 & 7.3775 & 238.2 & 65.3 & 61.4 & 30.04 & 1.05 & 36.2 \\\\\n",
       "\t10 & 7.8969 & 251.0 & 67.4 & 63.0 & 28.83 & 0.83 & 34.5 \\\\\n",
       "\t11 & 8.755 & 242.6 & 73.8 & 65.8 & 28.72 & 0.9 & 33.3 \\\\\n",
       "\t12 & 7.7309 & 248.2 & 60.3 & 64.2 & 30.13 & 1.23 & 44.6 \\\\\n",
       "\t13 & 9.4388 & 256.6 & 61.3 & 68.1 & 28.62 & 0.91 & 33.9 \\\\\n",
       "\t14 & 9.0225 & 265.5 & 59.8 & 63.8 & 28.14 & 0.9 & 39.1 \\\\\n",
       "\t15 & 7.1945 & 235.4 & 58.3 & 57.6 & 29.88 & 1.53 & 39.3 \\\\\n",
       "\t16 & 7.7147 & 240.8 & 71.4 & 59.7 & 28.48 & 0.74 & 41.7 \\\\\n",
       "\t17 & 8.3082 & 248.8 & 60.5 & 61.8 & 28.52 & 0.93 & 21.2 \\\\\n",
       "\t18 & 10.1185 & 251.1 & 70.6 & 69.1 & 28.11 & 0.91 & 48.4 \\\\\n",
       "\t19 & 7.0209 & 240.9 & 66.5 & 62.7 & 29.78 & 0.75 & 31.6 \\\\\n",
       "\t20 & 8.0647 & 253.6 & 64.3 & 62.4 & 28.46 & 1.01 & 42.6 \\\\\n",
       "\t21 & 7.7155 & 249.9 & 63.5 & 67.9 & 30.79 & 0.86 & 29.2 \\\\\n",
       "\t22 & 7.7808 & 249.0 & 74.0 & 65.0 & 30.2 & 0.76 & 32.4 \\\\\n",
       "\t23 & 7.0081 & 232.8 & 75.5 & 54.7 & 28.43 & 1.16 & 40.0 \\\\\n",
       "\t24 & 8.1518 & 242.5 & 69.3 & 60.9 & 27.78 & 1.0 & 37.0 \\\\\n",
       "\t25 & 7.5486 & 241.9 & 76.7 & 65.2 & 30.74 & 0.73 & 35.6 \\\\\n",
       "\t26 & 6.9462 & 228.1 & 70.4 & 59.0 & 30.22 & 1.0 & 51.4 \\\\\n",
       "\t27 & 9.8701 & 253.8 & 62.7 & 68.2 & 29.36 & 0.66 & 38.8 \\\\\n",
       "\t28 & 9.2063 & 246.3 & 69.0 & 66.4 & 28.17 & 0.81 & 37.3 \\\\\n",
       "\t29 & 10.6124 & 269.3 & 66.4 & 71.6 & 28.31 & 0.94 & 50.0 \\\\\n",
       "\t30 & 7.3346 & 238.7 & 70.6 & 58.3 & 29.08 & 0.86 & 43.9 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "78×7 DataFrame\n",
       "│ Row │ Column1 │ Column2 │ Column3 │ Column4 │ Column5 │ Column6 │ Column7 │\n",
       "│     │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1   │ 6.4673  │ 225.1   │ 78.2    │ 55.0    │ 29.47   │ 1.26    │ 47.9    │\n",
       "│ 2   │ 9.4293  │ 242.4   │ 65.5    │ 66.1    │ 28.51   │ 0.76    │ 23.4    │\n",
       "│ 3   │ 9.4986  │ 242.4   │ 70.7    │ 67.7    │ 28.64   │ 0.7     │ 48.4    │\n",
       "│ 4   │ 9.476   │ 257.1   │ 67.3    │ 68.2    │ 29.08   │ 0.77    │ 37.3    │\n",
       "│ 5   │ 7.6488  │ 238.1   │ 71.7    │ 57.2    │ 27.58   │ 0.95    │ 38.6    │\n",
       "│ 6   │ 7.3895  │ 261.1   │ 60.9    │ 62.3    │ 29.34   │ 1.02    │ 33.9    │\n",
       "│ 7   │ 9.1652  │ 261.1   │ 68.7    │ 65.3    │ 28.14   │ 1.14    │ 40.2    │\n",
       "│ 8   │ 7.4426  │ 252.0   │ 70.7    │ 62.3    │ 29.67   │ 1.16    │ 39.7    │\n",
       "│ 9   │ 7.3775  │ 238.2   │ 65.3    │ 61.4    │ 30.04   │ 1.05    │ 36.2    │\n",
       "│ 10  │ 7.8969  │ 251.0   │ 67.4    │ 63.0    │ 28.83   │ 0.83    │ 34.5    │\n",
       "⋮\n",
       "│ 68  │ 6.9257  │ 248.1   │ 65.2    │ 64.7    │ 30.52   │ 0.84    │ 34.0    │\n",
       "│ 69  │ 7.1127  │ 246.9   │ 52.2    │ 55.1    │ 27.56   │ 1.09    │ 37.5    │\n",
       "│ 70  │ 9.6066  │ 247.0   │ 70.0    │ 68.9    │ 28.53   │ 0.73    │ 47.9    │\n",
       "│ 71  │ 8.6925  │ 253.1   │ 59.2    │ 62.6    │ 29.16   │ 1.13    │ 45.1    │\n",
       "│ 72  │ 9.4762  │ 254.4   │ 67.1    │ 68.0    │ 28.19   │ 1.01    │ 33.7    │\n",
       "│ 73  │ 8.7267  │ 263.6   │ 68.2    │ 65.7    │ 28.58   │ 0.86    │ 37.9    │\n",
       "│ 74  │ 7.4237  │ 267.7   │ 67.2    │ 66.7    │ 30.8    │ 1.06    │ 28.3    │\n",
       "│ 75  │ 7.6477  │ 240.4   │ 78.3    │ 66.4    │ 30.42   │ 1.02    │ 31.8    │\n",
       "│ 76  │ 7.4479  │ 239.4   │ 65.7    │ 59.5    │ 29.17   │ 1.17    │ 30.9    │\n",
       "│ 77  │ 6.2003  │ 261.5   │ 62.7    │ 62.1    │ 30.59   │ 1.03    │ 40.0    │\n",
       "│ 78  │ 8.0792  │ 250.4   │ 68.4    │ 67.4    │ 29.01   │ 0.62    │ 20.5    │"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "train_data = CSV.read(\"Train lpga2008_opt.csv\",DataFrame; header=false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n",
      "Academic license - for non-commercial use only\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.0, -0.0, 1.0, 1.0, -0.0, -0.0], [0.0232123, 0.0, 0.200118, -0.356159, 0.0, 0.0], [1, 3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = Vector(train_data[:,1])\n",
    "X_train = Array{Float64,2}(train_data[:,2:end])\n",
    "s_opt, β,s_nonzeros  = SparseRegression(X_train,y_train,1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with LASSO\n",
    "\n",
    "Now that we have set this up, let us compare the speed and the quality of the solution with LASSO. To do so, we will use the Lasso regression function you coded in the previous recitation, tested agains the Sparse Regression method coded above.\n",
    "\n",
    "We will use simulated data to do the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       "  0.05023161516038392\n",
       "  0.17663664167377974\n",
       "  0.05261684292865973\n",
       "  0.21301142847894008\n",
       "  0.16932567199372933\n",
       "  0.22068937224277466\n",
       "  0.1554644960102942 \n",
       " -0.11331278812919703\n",
       "  0.1446216883617416 \n",
       "  0.15966403077481892"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining parameters\n",
    "n = 10\n",
    "p = 10\n",
    "k = 1\n",
    "β = zeros(p)\n",
    "\n",
    "# Picking sparse dimensions\n",
    "Random.seed!(15095)\n",
    "nonzeroinds = sample(1:p,k,replace=false)\n",
    "Random.seed!(15095)\n",
    "β[nonzeroinds] = rand(k)\n",
    "\n",
    "# Generating the Data\n",
    "Random.seed!(15095)\n",
    "X = rand(n,p)\n",
    "Random.seed!(15095)\n",
    "y = X * β + rand(Normal(0,0.1),n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n",
      "Academic license - for non-commercial use only\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.213629], [10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_opt, β_opt = SparseRegression(X,y,100,k)\n",
    "#s_nonzeros = findall(x -> x>0.5, s_opt)\n",
    "#println(sort(s_nonzeros))\n",
    "#println(sort(nonzeroinds))\n",
    "#println(mean((abs.(β-β_opt)./β)[nonzeroinds]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Sparse Regression\n",
    "\n",
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n",
      "[10]\n",
      "0.09161686550545232\n",
      "Sparse regression finds the coefficients [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.213629]\n",
      "The real coefficients are [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.235175]\n"
     ]
    }
   ],
   "source": [
    "s_nonzeros = findall(x -> x>0.5, s_opt)\n",
    "println(sort(s_nonzeros))\n",
    "println(sort(nonzeroinds))\n",
    "println(mean((abs.(β-β_opt)./β)[nonzeroinds]))\n",
    "println(\"Sparse regression finds the coefficients \",β_opt)\n",
    "println(\"The real coefficients are \",β)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n",
      "Academic license - for non-commercial use only\n",
      "  0.015363 seconds (8.52 k allocations: 386.453 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.213629], [10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time SparseRegression(X,y,100,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the Lasso regression function using the previous recitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l1_regression (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function l1_regression(X, y, rho; solver_output=0)\n",
    "    n,p = size(X)\n",
    "    \n",
    "    # Build model\n",
    "    model = Model(Gurobi.Optimizer)\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", solver_output) \n",
    "    \n",
    "    # Insert variables and constraints\n",
    "    @variable(model,beta[j=1:p])\n",
    "    @variable(model,beta_abs[j=1:p]>=0)\n",
    "    @variable(model, sse>=0)\n",
    "    @constraint(model,[j=1:p], beta[j]<=beta_abs[j])\n",
    "    @constraint(model,[j=1:p], beta[j]>=-beta_abs[j])\n",
    "    @constraint(model, sum((y[i]-sum(X[i,j]*beta[j] for j=1:p))^2 for i=1:n) <= sse)\n",
    "    @objective(model,Min, sse + rho*sum(beta_abs[j] for j=1:p))\n",
    "    \n",
    "    # Optimize\n",
    "    optimize!(model)\n",
    "    \n",
    "    # Return estimated betas\n",
    "    return (value.(beta))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n",
      "Academic license - for non-commercial use only\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 4.1210382477950615e-10\n",
       " 6.455406863317954e-10 \n",
       " 2.921521154348343e-9  \n",
       " 2.5230180511657014e-9 \n",
       " 2.0947335613004685e-9 \n",
       " 0.01778555064230097   \n",
       " 1.7515418661040304e-9 \n",
       " 1.90371305175229e-9   \n",
       " 1.3788150059355628e-9 \n",
       " 0.08392324035684719   "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_lasso = l1_regression( X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "0.643144906484792\n"
     ]
    }
   ],
   "source": [
    "s_nonzeros = findall(x -> abs.(x)>0, beta_lasso)\n",
    "println(sort(s_nonzeros))\n",
    "println(mean((abs.(β-beta_lasso)./β)[nonzeroinds]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare this back to the sparse regression formulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09161686550545232\n"
     ]
    }
   ],
   "source": [
    "println(mean((abs.(β-β_opt)./β)[nonzeroinds]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, lasso fails to recover the true coefficients and results in a large error, while sparse regression pretty much extracts the exact true solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n",
      "Academic license - for non-commercial use only\n",
      "  0.160892 seconds (265.30 k allocations: 13.393 MiB, 7.29% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 3.213437752145101e-9 \n",
       " 7.167077745923063e-9 \n",
       " 2.5394354229264988e-8\n",
       " 4.2966141093543934e-8\n",
       " 3.9223465661398344e-8\n",
       " 1.0959548320103353e-7\n",
       " 2.5567934132299935e-8\n",
       " 1.979773980874511e-8 \n",
       " 1.751294166754463e-8 \n",
       " 1.0677548893917954e-7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time l1_regression( X, y, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare this back to the sparse regression formulation again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only\n",
      "Academic license - for non-commercial use only\n",
      "  0.009731 seconds (8.51 k allocations: 386.406 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.213629], [10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time SparseRegression(X,y,100,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The times for these functions can actually vary across different runs, but generally we see that the Lasso is either slower than or runs at a comparable speed to sparse regression. Sparse regression, however, gets much closer to recovering the true coefficient compared to Lasso. Try out different combination of $n$, $p$, and $k$ for yourself to see what works best for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
